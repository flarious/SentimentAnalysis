{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140 = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding=\"ISO-8859-1\", names=[\"sentiment\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "sentiment140 = sentiment140[['text', 'sentiment']]\n",
    "sentiment140['sentiment'] = sentiment140['sentiment'].replace(0, \"negative\")\n",
    "sentiment140['sentiment'] = sentiment140['sentiment'].replace(4, \"postive\")\n",
    "sentiment140 = sentiment140.dropna(axis=0, how=\"any\")\n",
    "sentiment140.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwitterTweetSentiment = pd.read_csv('Tweets.csv')\n",
    "TwitterTweetSentiment = TwitterTweetSentiment[['text', 'sentiment']]\n",
    "TwitterTweetSentiment = TwitterTweetSentiment[TwitterTweetSentiment['sentiment'] != \"neutral\"]\n",
    "TwitterTweetSentiment = TwitterTweetSentiment.dropna(axis=0, how=\"any\")\n",
    "TwitterTweetSentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwitterSentimentAnalysis = pd.read_csv('twitter_training.csv', names=[\"id\", \"entity\", \"sentiment\", \"text\"])\n",
    "TwitterSentimentAnalysis = TwitterSentimentAnalysis[['text', 'sentiment']]\n",
    "TwitterSentimentAnalysis['sentiment'] = TwitterSentimentAnalysis['sentiment'].str.lower()\n",
    "TwitterSentimentAnalysis = TwitterSentimentAnalysis[TwitterSentimentAnalysis['sentiment'] != \"neutral\"]\n",
    "TwitterSentimentAnalysis = TwitterSentimentAnalysis[TwitterSentimentAnalysis['sentiment'] != \"irrelevant\"]\n",
    "TwitterSentimentAnalysis = TwitterSentimentAnalysis.dropna(axis=0, how=\"any\")\n",
    "TwitterSentimentAnalysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessText(text):\n",
    "    processedText = []\n",
    "\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "\n",
    "    for tweet in text:\n",
    "        # Lower Casing\n",
    "        tweet = tweet.lower()\n",
    "\n",
    "        # Replacing URL\n",
    "        tweet = re.sub(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\", ' URL', tweet)\n",
    "\n",
    "        # Replacing Emoji\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])\n",
    "\n",
    "        # Replacing Usernames\n",
    "        tweet = re.sub(\"@[^\\s]+\", \" USER\", tweet)\n",
    "\n",
    "        # Removing Non-alphabets\n",
    "        tweet = re.sub(\"[^a-zA-Z0-9]\", \" \", tweet)\n",
    "\n",
    "        # Removing Consecutive letters\n",
    "        tweet = re.sub(r\"(.)\\1\\1\", r\"\\1\\1\", tweet)\n",
    "\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            # Removing Stopwords\n",
    "            if word not in stopwordlist:\n",
    "                # Removing Short words\n",
    "                if len(word) > 1:\n",
    "                    # Lemmatazing words\n",
    "                    word = wordLemm.lemmatize(word)\n",
    "                    tweetwords += (word+' ')\n",
    "\n",
    "        processedText.append(tweetwords)\n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedS140 = preprocessText(sentiment140['text'])\n",
    "processedTTS = preprocessText(TwitterTweetSentiment['text'])\n",
    "processedTSA = preprocessText(TwitterSentimentAnalysis['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainS140, X_testS140, y_trainS140, y_testS140 = train_test_split(processedS140, sentiment140['sentiment'], test_size=0.2)\n",
    "X_trainTTS, X_testTTS, y_trainTTS, y_testTTS = train_test_split(processedTTS, TwitterTweetSentiment['sentiment'], test_size=0.2)\n",
    "X_trainTSA, X_testTSA, y_trainTSA, y_testTSA = train_test_split(processedTSA, TwitterSentimentAnalysis['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,3), max_features=500000)\n",
    "vectoriser.fit(X_trainS140)\n",
    "X_trainS140 = vectoriser.transform(X_trainS140)\n",
    "X_testS140 = vectoriser.transform(X_testS140)\n",
    "\n",
    "vectoriser.fit(X_trainTTS)\n",
    "X_trainTTS = vectoriser.transform(X_trainTTS)\n",
    "X_testTTS = vectoriser.transform(X_testTTS)\n",
    "\n",
    "vectoriser.fit(X_trainTSA)\n",
    "X_trainTSA = vectoriser.transform(X_trainTSA)\n",
    "X_testTSA = vectoriser.transform(X_testTSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    confusion = confusion_matrix(y, y_pred)\n",
    "\n",
    "    categories = ['Negative', 'Positive']\n",
    "    group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in confusion.flatten() / np.sum(confusion)]\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "    sns.heatmap(confusion, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"Actual values\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_acc(model, X_train, X_test, y_train, y_test):\n",
    "    print(\"+-+ Training Accuracy +-+\")\n",
    "    evaluate_model(model, X_train, y_train)\n",
    "\n",
    "    print(\"+-+ Testing Accuracy +--+\")\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_test_acc(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNBmodel = BernoulliNB(alpha = 2)\n",
    "SVCmodel = LinearSVC()\n",
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "RFmodel = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+===+ Sentiment140 Dataset +===+\")\n",
    "print(\"+---+ Bernoulli Naive Bayes +---+\")\n",
    "train_test_model(BNBmodel, X_trainS140, X_testS140, y_trainS140, y_testS140)\n",
    "print(\"+---+ Linear Support Vector Classification +---+\")\n",
    "train_test_model(SVCmodel, X_trainS140, X_testS140, y_trainS140, y_testS140)\n",
    "print(\"+---+ Logisitic Regression +---+\")\n",
    "train_test_model(LRmodel, X_trainS140, X_testS140, y_trainS140, y_testS140)\n",
    "print(\"+---+ Random Forest +---+\")\n",
    "train_test_model(RFmodel, X_trainS140, X_testS140, y_trainS140, y_testS140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+===+ Twitter Tweets Sentiment Dataset +===+\")\n",
    "print(\"+---+ Bernoulli Naive Bayes +---+\")\n",
    "train_test_model(BNBmodel, X_trainTTS, X_testTTS, y_trainTTS, y_testTTS)\n",
    "print(\"+---+ Linear Support Vector Classification +---+\")\n",
    "train_test_model(SVCmodel, X_trainTTS, X_testTTS, y_trainTTS, y_testTTS)\n",
    "print(\"+---+ Logisitic Regression +---+\")\n",
    "train_test_model(LRmodel, X_trainTTS, X_testTTS, y_trainTTS, y_testTTS)\n",
    "print(\"+---+ Random Forest +---+\")\n",
    "train_test_model(RFmodel, X_trainTTS, X_testTTS, y_trainTTS, y_testTTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+===+ Twitter Sentiment Analysis Dataset +===+\")\n",
    "print(\"+---+ Bernoulli Naive Bayes +---+\")\n",
    "train_test_model(BNBmodel, X_trainTSA, X_testTSA, y_trainTSA, y_testTSA)\n",
    "print(\"+---+ Linear Support Vector Classification +---+\")\n",
    "train_test_model(SVCmodel, X_trainTSA, X_testTSA, y_trainTSA, y_testTSA)\n",
    "print(\"+---+ Logisitic Regression +---+\")\n",
    "train_test_model(LRmodel, X_trainTSA, X_testTSA, y_trainTSA, y_testTSA)\n",
    "print(\"+---+ Random Forest +---+\")\n",
    "train_test_model(RFmodel, X_trainTSA, X_testTSA, y_trainTSA, y_testTSA)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51ed8effa77163ac3c6265915571490a4b0196897aa526145bdb641399b7a218"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
